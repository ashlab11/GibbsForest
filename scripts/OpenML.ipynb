{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from openml.tasks import list_tasks, TaskType\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from src import GibbsForest\n",
    "import logging\n",
    "\n",
    "\n",
    "# Load the OpenML dataset\n",
    "regression_tasks = list_tasks(task_type = TaskType.SUPERVISED_REGRESSION)\n",
    "classification_tasks = list_tasks(task_type = TaskType.SUPERVISED_CLASSIFICATION, size = 100)\n",
    "small_tasks_ids = []\n",
    "for task_id, task_value in regression_tasks.items():\n",
    "    if ('NumberOfInstances' in task_value.keys() and task_value['NumberOfInstances'] < 10000 and task_value['NumberOfInstances'] > 5000 and \n",
    "    'NumberOfMissingValues' in task_value.keys() and task_value['NumberOfMissingValues'] == 0 and \n",
    "    'NumberOfSymbolicFeatures' in task_value.keys() and task_value['NumberOfSymbolicFeatures'] == 0):\n",
    "        small_tasks_ids.append(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"openml.extensions.sklearn.extension\").setLevel(logging.ERROR)\n",
    "if not hasattr(GibbsForest, '__version__'):\n",
    "    GibbsForest.__version__ = \"0.0.1\"  # Use an appropriate version number\n",
    "\n",
    "task_id = small_tasks_ids[0]\n",
    "task = openml.tasks.get_task(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 1.3482\n",
      "Test error: 1.6894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from src import Losses\n",
    "\n",
    "#task_id = small_tasks_ids[0]\n",
    "#task = openml.tasks.get_task(task_id)\n",
    "#X, y = task.get_X_and_y()\n",
    "\n",
    "X, y = make_friedman1(n_samples = 10000, n_features = 10, noise = 2)\n",
    "\n",
    "gibbs_params = {\"eta\": 0.1,\n",
    "            \"feature_subsample\": 0.6,\n",
    "            \"max_depth\": 5,\n",
    "            \"min_samples\": 2,\n",
    "            \"n_trees\": 80, \n",
    "            'row_subsample': 0.9, \n",
    "            'warmup_depth': 2, \n",
    "            'loss_fn': Losses.LeastSquaresLoss()}\n",
    "\n",
    "dyna = GibbsForest(**gibbs_params)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.1, \n",
    "    'subsample': 0.9, \n",
    "    'max_depth': 5\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\"\"\"rf_params = {\n",
    "            \"max_depth\": 5,\n",
    "            \"max_features\": 0.50,\n",
    "            \"min_samples_leaf\": 3,\n",
    "            \"n_estimators\": 68}\n",
    "dyna = RandomForestRegressor(**rf_params)\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "xgb.fit(X_train, y_train)\n",
    "train_error = mean_absolute_error(y_train, xgb.predict(X_train))\n",
    "test_error = mean_absolute_error(y_test, xgb.predict(X_test))\n",
    "print(f\"Train error: {train_error:.4f}\")\n",
    "print(f\"Test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trees \u001b[38;5;241m=\u001b[39m dyna\u001b[38;5;241m.\u001b[39m_trees\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [tree\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m trees]\n\u001b[0;32m----> 3\u001b[0m tree_biases \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(predictions \u001b[38;5;241m-\u001b[39m y_test, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m correlations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(trees), \u001b[38;5;28mlen\u001b[39m(trees)))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trees)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "trees = dyna._trees\n",
    "predictions = [tree.predict(X_test) for tree in trees]\n",
    "tree_biases = np.mean(predictions - y_test, axis = 1)\n",
    "correlations = np.zeros((len(trees), len(trees)))\n",
    "for i in range(len(trees)):\n",
    "    for j in range(i, len(trees)):\n",
    "        correlations[i, j] = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "        correlations[j, i] = correlations[i, j]\n",
    "print(f\"Correlations: {np.mean(correlations)}\")\n",
    "print(f\"Tree biases: {np.mean(tree_biases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = dyna.estimators_\n",
    "predictions = [tree.predict(X_test) for tree in trees]\n",
    "tree_biases = np.mean(predictions - y_test, axis = 1)\n",
    "correlations = np.zeros((len(trees), len(trees)))\n",
    "for i in range(len(trees)):\n",
    "    for j in range(i, len(trees)):\n",
    "        correlations[i, j] = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "        correlations[j, i] = correlations[i, j]\n",
    "print(f\"Correlations: {np.mean(correlations)}\")\n",
    "print(f\"Tree biases: {np.mean(tree_biases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------------------------------\n",
    "# Assumptions:\n",
    "#   - dyna: your trained random forest model (e.g., RandomForestRegressor)\n",
    "#           with an attribute `estimators_` (list of individual tree models)\n",
    "#   - X_test: test features (numpynp.array or similar)\n",
    "#   - y_test: true test targets (numpynp.array)\n",
    "#\n",
    "# Replace the following dummy definitions with your actual data/model.\n",
    "# -------------------------------\n",
    "# Example (comment these out if you already have these defined):\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "#\n",
    "# X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# dyna = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "# dyna.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Compute Predictions for Each Tree\n",
    "# -------------------------------\n",
    "trees = dyna._trees\n",
    "n_trees = len(trees)\n",
    "# Each tree produces predictions on the test set; shape: (n_trees, n_samples)\n",
    "predictions = np.array([tree.predict(X_test) for tree in trees])\n",
    "\n",
    "# Ensemble prediction is the average over trees:\n",
    "ensemble_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute Residuals (Errors)\n",
    "# -------------------------------\n",
    "# Residuals for each tree and for the ensemble:\n",
    "residuals = predictions - y_test  # shape: (n_trees, n_samples)\n",
    "ensemble_residual = ensemble_pred - y_test\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Correlation Among Tree Predictions\n",
    "# -------------------------------\n",
    "# Compute the correlation matrix among the treesâ€™ predictions.\n",
    "correlation_matrix = np.corrcoef(predictions, rowvar=False)\n",
    "# Compute the mean correlation (using only the off-diagonal values)\n",
    "upper_tri_indices = np.triu_indices(n_trees, k=1)\n",
    "mean_correlation = np.mean(correlation_matrix[upper_tri_indices])\n",
    "print(\"Mean correlation among tree predictions: {:.4f}\".format(mean_correlation))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Correlation Among Tree Residuals\n",
    "# -------------------------------\n",
    "resid_corr_matrix = np.corrcoef(residuals)\n",
    "mean_resid_corr = np.mean(resid_corr_matrix[upper_tri_indices])\n",
    "print(\"Mean correlation among tree residuals: {:.4f}\".format(mean_resid_corr))\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Compute MSE and MAE for Each Tree and the Ensemble\n",
    "# -------------------------------\n",
    "tree_mse = np.array([mean_squared_error(y_test, predictions[i]) for i in range(n_trees)])\n",
    "tree_mae = np.array([mean_absolute_error(y_test, predictions[i]) for i in range(n_trees)])\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "\n",
    "print(\"Average individual tree MSE: {:.4f}\".format(np.mean(tree_mse)))\n",
    "print(\"Ensemble MSE: {:.4f}\".format(ensemble_mse))\n",
    "print(\"Average individual tree MAE: {:.4f}\".format(np.mean(tree_mae)))\n",
    "print(\"Ensemble MAE: {:.4f}\".format(ensemble_mae))\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Compute Prediction Variance Across Trees (Per Sample)\n",
    "# -------------------------------\n",
    "# For each test sample, compute variance among the tree predictions\n",
    "variance_per_sample = np.var(predictions, axis=0)\n",
    "avg_prediction_variance = np.mean(variance_per_sample)\n",
    "print(\"Average variance of tree predictions across samples: {:.4f}\".format(avg_prediction_variance))\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Ambiguity Decomposition (Regression)\n",
    "# -------------------------------\n",
    "# One way to look at ensemble benefits is via the ambiguity decomposition:\n",
    "#   Ensemble MSE = Average tree MSE - Ambiguity\n",
    "ambiguity = np.mean(tree_mse) - ensemble_mse\n",
    "print(\"Ambiguity (Average tree MSE - Ensemble MSE): {:.4f}\".format(ambiguity))\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Feature Importances (If Available)\n",
    "# -------------------------------\n",
    "# If your individual trees have a `feature_importances_` attribute,\n",
    "# compute the average importance for each feature.\n",
    "try:\n",
    "    feature_importances = np.array([tree.feature_importances_ for tree in trees])\n",
    "    avg_feature_importance = np.mean(feature_importances, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(avg_feature_importance)), avg_feature_importance, color='skyblue')\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Average Feature Importance\")\n",
    "    plt.title(\"Average Feature Importances Across Trees\")\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    print(\"Not all trees have a 'feature_importances_' attribute.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Plot Distribution of Residuals\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot ensemble residual distribution\n",
    "sns.histplot(ensemble_residual, color=\"blue\", label=\"Ensemble Residual\", kde=True, stat=\"density\", bins=30)\n",
    "\n",
    "# Plot residual distributions for a few individual trees (first 3 trees)\n",
    "for i in [38, 39]:\n",
    "    sns.histplot(residuals[i], kde=True, stat=\"density\", bins=30, label=f\"Tree {i} Residual\", alpha=0.6)\n",
    "    \n",
    "plt.xlabel(\"Residual (Prediction - True Value)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Residual Distributions: Ensemble vs. Individual Trees\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Correlation Heatmap for Tree Predictions\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Heatmap of Tree Predictions Correlations\")\n",
    "plt.xlabel(\"Tree Index\")\n",
    "plt.ylabel(\"Tree Index\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Correlation Heatmap for Tree Residuals\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(resid_corr_matrix, cmap=\"coolwarm\", square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Heatmap of Tree Residuals Correlations\")\n",
    "plt.xlabel(\"Tree Index\")\n",
    "plt.ylabel(\"Tree Index\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Boxplots for Individual Tree MSE and MAE\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=tree_mse)\n",
    "plt.title(\"Boxplot of Individual Tree MSE\")\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=tree_mae)\n",
    "plt.title(\"Boxplot of Individual Tree MAE\")\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_counts = [tree.num_splits for tree in dyna._trees]\n",
    "plt.hist(split_counts, bins=range(0, max(split_counts)+2, 2))\n",
    "plt.title(\"Distribution of Splits per Tree\")\n",
    "plt.xlabel(\"Number of Splits\")\n",
    "plt.ylabel(\"Count of Trees\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = dyna._trees\n",
    "predictions = np.array([tree.predict(X_train) for tree in trees])\n",
    "tree_mse = np.array([mean_squared_error(y_train, predictions[i]) for i in range(len(trees))])\n",
    "best_tree_idxs = np.argsort(tree_mse)\n",
    "errors = []\n",
    "for num_trees in range(2, 79):    \n",
    "    tree_list = [trees[i] for i in best_tree_idxs][:num_trees]\n",
    "    predictions_test = np.array([tree.predict(X_test) for tree in tree_list])\n",
    "    ensemble_pred = np.mean(predictions_test, axis=0)\n",
    "    ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "    ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "    errors.append(ensemble_mae)\n",
    "\n",
    "# Plot the error as a function of the number of trees\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 79), errors, marker='o', color='blue')\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Ensemble MAE\")\n",
    "plt.title(\"Ensemble MAE vs. Number of Trees\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "print(xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
