{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from openml.tasks import list_tasks, TaskType\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from src import GibbsForest\n",
    "import logging\n",
    "\n",
    "\n",
    "# Load the OpenML dataset\n",
    "regression_tasks = list_tasks(task_type = TaskType.SUPERVISED_REGRESSION)\n",
    "small_tasks_ids = []\n",
    "for task_id, task_value in regression_tasks.items():\n",
    "    \"\"\"We want datasets with instances between 5000 and 10000, no missing values, and no symbolic features\"\"\"\n",
    "    if ('NumberOfInstances' in task_value.keys() and task_value['NumberOfInstances'] < 10000 and task_value['NumberOfInstances'] > 5000 and\n",
    "    'NumberOfMissingValues' in task_value.keys() and task_value['NumberOfMissingValues'] == 0 and \n",
    "    'NumberOfSymbolicFeatures' in task_value.keys() and task_value['NumberOfSymbolicFeatures'] == 0):\n",
    "        small_tasks_ids.append(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"openml.extensions.sklearn.extension\").setLevel(logging.ERROR)\n",
    "if not hasattr(GibbsForest, '__version__'):\n",
    "    GibbsForest.__version__ = \"0.0.1\"  # Use an appropriate version number\n",
    "\n",
    "print(len(small_tasks_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y mean: 0.7143\n",
      "Hessian difference:  0.0\n",
      "Hessian difference:  0.0\n",
      "Hessian difference:  0.0\n",
      "Hessian difference:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"rf_params = {\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m            \"max_depth\": 5,\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m            \"max_features\": 0.50,\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m            \"min_samples_leaf\": 3,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m            \"n_estimators\": 68}\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mdyna = RandomForestRegressor(**rf_params)\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m dyna\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     43\u001b[0m train_error \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_train, dyna\u001b[38;5;241m.\u001b[39mpredict(X_train))\n\u001b[1;32m     44\u001b[0m test_error \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, dyna\u001b[38;5;241m.\u001b[39mpredict(X_test))\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/GibbsForest.py:115\u001b[0m, in \u001b[0;36mGibbsForest.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    112\u001b[0m predictions_without_tree \u001b[38;5;241m=\u001b[39m weights_without_tree \u001b[38;5;241m@\u001b[39m individual_predictions_without_tree\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#Get best split with these weights\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m error_reduction, best_split \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mget_best_split(X_batch, y_batch, predictions_without_tree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[tree_idx], \n\u001b[1;32m    116\u001b[0m                                                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_reduction \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_gamma: \u001b[38;5;66;03m#Only split if gain is above gamma\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     tree\u001b[38;5;241m.\u001b[39msplit(X_batch, y_batch)\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/Tree.py:21\u001b[0m, in \u001b[0;36mTree.get_best_split\u001b[0;34m(self, X, y, other_predictions, tree_weight, eta)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_best_split\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, other_predictions, tree_weight, eta):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mget_best_split(X, y, other_predictions, tree_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_considered, eta)\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/LeafOrNode.py:36\u001b[0m, in \u001b[0;36mLeafOrNode.get_best_split\u001b[0;34m(self, X, y, other_predictions, tree_weight, features_to_consider, eta)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_split_leaf(X, y, other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_split_node(X, y, other_predictions, tree_weight, features_to_consider, eta)\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/LeafOrNode.py:51\u001b[0m, in \u001b[0;36mLeafOrNode.get_best_split_node\u001b[0;34m(self, X, y, other_predictions, tree_weight, features_to_consider, eta)\u001b[0m\n\u001b[1;32m     48\u001b[0m left_y \u001b[38;5;241m=\u001b[39m y[left_indices]\n\u001b[1;32m     49\u001b[0m right_y \u001b[38;5;241m=\u001b[39m y[right_indices]\n\u001b[0;32m---> 51\u001b[0m left_best_error_reduction, left_col_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39mget_best_split(left_X, left_y, left_other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     52\u001b[0m right_best_error_reduction, right_col_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mget_best_split(right_X, right_y, right_other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_best_error_reduction \u001b[38;5;241m>\u001b[39m right_best_error_reduction:\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/LeafOrNode.py:36\u001b[0m, in \u001b[0;36mLeafOrNode.get_best_split\u001b[0;34m(self, X, y, other_predictions, tree_weight, features_to_consider, eta)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_split_leaf(X, y, other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_split_node(X, y, other_predictions, tree_weight, features_to_consider, eta)\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/LeafOrNode.py:51\u001b[0m, in \u001b[0;36mLeafOrNode.get_best_split_node\u001b[0;34m(self, X, y, other_predictions, tree_weight, features_to_consider, eta)\u001b[0m\n\u001b[1;32m     48\u001b[0m left_y \u001b[38;5;241m=\u001b[39m y[left_indices]\n\u001b[1;32m     49\u001b[0m right_y \u001b[38;5;241m=\u001b[39m y[right_indices]\n\u001b[0;32m---> 51\u001b[0m left_best_error_reduction, left_col_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39mget_best_split(left_X, left_y, left_other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     52\u001b[0m right_best_error_reduction, right_col_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mget_best_split(right_X, right_y, right_other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_best_error_reduction \u001b[38;5;241m>\u001b[39m right_best_error_reduction:\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/LeafOrNode.py:34\u001b[0m, in \u001b[0;36mLeafOrNode.get_best_split\u001b[0;34m(self, X, y, other_predictions, tree_weight, features_to_consider, eta)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#No error reduction possible\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_split_leaf(X, y, other_predictions, tree_weight, features_to_consider, eta)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_split_node(X, y, other_predictions, tree_weight, features_to_consider, eta)\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/LeafOrNode.py:68\u001b[0m, in \u001b[0;36mLeafOrNode.get_best_split_leaf\u001b[0;34m(self, X, y, other_predictions, tree_weight, features_to_consider, eta)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(other_predictions)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAN in other_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m gain, col, splitting_val, left_val, right_val \u001b[38;5;241m=\u001b[39m find_split(\n\u001b[1;32m     69\u001b[0m     X, y, other_predictions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval, tree_weight, features_to_consider \u001b[38;5;241m=\u001b[39m features_to_consider, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples,\n\u001b[1;32m     70\u001b[0m     eta \u001b[38;5;241m=\u001b[39m eta, loss_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn, initial_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_weight)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_best_col \u001b[38;5;241m=\u001b[39m col\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_best_splitting_val \u001b[38;5;241m=\u001b[39m splitting_val\n",
      "File \u001b[0;32m~/Documents/Dynatree/src/scan_thresholds.py:82\u001b[0m, in \u001b[0;36mfind_split\u001b[0;34m(X, y, other_predictions, leaf_weight, tree_weight, features_to_consider, loss_fn, min_samples, eta, reg_lambda, initial_weight, eps)\u001b[0m\n\u001b[1;32m     77\u001b[0m h_i_sorted \u001b[38;5;241m=\u001b[39m h_i[sort_idx]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#We only want to consider the splits when y is at the end of an X value, not in the middle\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#We also care about unique values, so we need to find the last unique index -- this does both\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#Min samples 2, [1, 1, 1, 1, 2, 2, 3, 3] -> [3, 5]\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m placements \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msearchsorted(X_col_sorted, X_unique_sorted, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     83\u001b[0m placements_correct \u001b[38;5;241m=\u001b[39m [placement \u001b[38;5;28;01mfor\u001b[39;00m placement \u001b[38;5;129;01min\u001b[39;00m placements \u001b[38;5;28;01mif\u001b[39;00m placement \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (min_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m placement \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_col_sorted) \u001b[38;5;241m-\u001b[39m min_samples]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(placements_correct) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# No valid splits for this column given the min_samples restriction.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1413\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \n\u001b[1;32m   1412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchsorted\u001b[39m\u001b[38;5;124m'\u001b[39m, v, side\u001b[38;5;241m=\u001b[39mside, sorter\u001b[38;5;241m=\u001b[39msorter)\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src import Losses\n",
    "\n",
    "task_id = small_tasks_ids[0]\n",
    "task = openml.tasks.get_task(task_id)\n",
    "X, y = task.get_X_and_y()\n",
    "print(f\"Y mean: {y.mean():.4f}\")\n",
    "\n",
    "gibbs_params = {\"eta\": 0.1,\n",
    "            \"feature_subsample\": 0.6,\n",
    "            \"max_depth\": 5,\n",
    "            \"min_samples\": 2,\n",
    "            \"n_trees\": 80, \n",
    "            'row_subsample': 0.9, \n",
    "            'warmup_depth': 2, \n",
    "            'loss_fn': Losses.LeastSquaresLoss(), \n",
    "            'reg_lambda': 0,\n",
    "            'reg_gamma': 1, \n",
    "            'tree_eta': 0}\n",
    "\n",
    "dyna = GibbsForest(**gibbs_params)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.1, \n",
    "    'subsample': 0.99, \n",
    "    'max_depth': 5, \n",
    "    'reg_lambda': 0.1, \n",
    "    'gamma': 1\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\"\"\"rf_params = {\n",
    "            \"max_depth\": 5,\n",
    "            \"max_features\": 0.50,\n",
    "            \"min_samples_leaf\": 3,\n",
    "            \"n_estimators\": 68}\n",
    "dyna = RandomForestRegressor(**rf_params)\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "dyna.fit(X_train, y_train)\n",
    "train_error = mean_absolute_error(y_train, dyna.predict(X_train))\n",
    "test_error = mean_absolute_error(y_test, dyna.predict(X_test))\n",
    "print(f\"Train error: {train_error:.4f}\")\n",
    "print(f\"Test error: {test_error:.4f}\")\n",
    "\"\"\"booster = dyna.get_booster()\n",
    "df = booster.trees_to_dataframe()\n",
    "total_leaves = (df[\"Feature\"] == \"Leaf\").sum()\n",
    "print(f\"Total leaves: {total_leaves}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98775999 1.77699225 0.66721119 1.00201811 2.28872569 0.94596383\n",
      " 0.63589012 1.07628623 1.54783305 0.79208905 0.74499233 0.56543082\n",
      " 0.80326761 0.7139758  0.92597692 1.41672049 0.81350755 0.71557637\n",
      " 0.95150385 0.8581724  0.58214081 1.04269012 0.99693711 0.74342795\n",
      " 1.77873845 0.83307168 1.09889593 0.86808775 1.07970167 0.91286441\n",
      " 1.37454095 1.012785   0.80253978 0.87384331 1.27863629 0.49310881\n",
      " 0.55493676 0.64986764 1.045762   1.17369679 0.92350545 1.42583611\n",
      " 1.17550495 1.53729868 0.53583756 0.9485519  0.73522861 0.9420546\n",
      " 0.83194482 0.96990376 2.1477189  0.3591652  1.56310618 0.93524986\n",
      " 1.12526052 1.62720905 0.75537197 1.28528126 0.26325736 0.71076507\n",
      " 0.61119699 0.94920652 0.80108407 1.78303779 1.38776974 0.85217331\n",
      " 1.08157065 0.96861443 0.77428573 1.62307457 0.95388184 1.37656909\n",
      " 0.7687903  0.91909319 0.59179642 0.64389382 0.70578118 0.86644852\n",
      " 1.31169751 0.80581564]\n"
     ]
    }
   ],
   "source": [
    "print(dyna.weights * len(dyna._trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([14, 30, 36, 38, 42, 46]),)\n",
      "Errors for Random forest: 1.3007481757745454\n",
      "Errors for XGBoost: 1.1813441520618182\n",
      "Errors for Gibbs Forest: 1.1316849013131818\n",
      "Fraction of datasets Gibbs Forest is better than Random Forest: 0.7954545454545454\n",
      "Fraction of datasets Gibbs Forest is better than XGBoost: 0.8636363636363636\n",
      "Average relative improvement of Gibbs Forest over Random Forest: 0.1256627863698241\n",
      "Average relative improvement of Gibbs Forest over XGBoost: 0.05585898940360959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "errors = np.array([[1.55012961e-01, 1.19690567e-01, 1.18715062e-01],\n",
    " [2.27864006e+00, 1.63303653e+00, 1.59774050e+00],\n",
    " [1.11279170e-03, 1.30388785e-03, 1.12549638e-03],\n",
    " [2.61028864e+00, 2.56309775e+00, 2.53819391e+00],\n",
    " [2.54181707e+00, 2.28245487e+00, 1.87539849e+00],\n",
    " [5.17212119e-01, 4.22303067e-01, 4.00673763e-01],\n",
    " [5.00497202e-01, 4.19176071e-01, 3.75706172e-01],\n",
    " [4.87178472e-01, 4.36459287e-01, 3.70631891e-01],\n",
    " [5.61965561e-01, 5.18073543e-01, 4.96628538e-01],\n",
    " [1.07452048e-01, 1.08739641e-01, 1.08610724e-01],\n",
    " [8.42492473e-03, 9.02710288e-03, 8.74388075e-03],\n",
    " [1.85544239e-02, 1.86710448e-02, 1.86979661e-02],\n",
    " [1.93822924e-02, 1.93739274e-02, 1.93643104e-02],\n",
    " [2.66070075e+00, 2.47643256e+00, 2.43504228e+00],\n",
    " [1.00000000e+02, 1.00000000e+02, 1.00000000e+02],\n",
    " [5.87860977e-02, 5.53437466e-02, 5.47524916e-02],\n",
    " [2.52429177e+00, 2.10119247e+00, 1.90582350e+00],\n",
    " [2.96629201e-02, 2.45982695e-02, 2.26927044e-02],\n",
    " [2.37716708e+00, 1.73599986e+00, 1.56691770e+00],\n",
    " [1.53521432e-01, 1.14978454e-01, 1.09412079e-01],\n",
    " [2.42959334e+00, 1.68032178e+00, 1.69157321e+00],\n",
    " [1.06947566e-03, 1.27401975e-03, 1.12249952e-03],\n",
    " [2.55821045e+00, 2.54205453e+00, 2.56533770e+00],\n",
    " [2.48475628e+00, 2.07870051e+00, 1.90476740e+00],\n",
    " [5.29935374e-01, 5.12014280e-01, 4.84926766e-01],\n",
    " [1.09046146e-01, 1.09390472e-01, 1.11111156e-01],\n",
    " [8.44670889e-03, 8.59229324e-03, 7.76171153e-03],\n",
    " [1.98315069e-02, 1.99174567e-02, 1.98971291e-02],\n",
    " [1.99302548e-02, 1.99821859e-02, 2.00077061e-02],\n",
    " [2.70376614e+00, 2.35912366e+00, 2.37085021e+00],\n",
    " [1.00000000e+02, 1.00000000e+02, 1.00000000e+02],\n",
    " [5.73799461e-02, 5.55275076e-02, 5.38979819e-02],\n",
    " [2.62285960e+00, 2.23544209e+00, 1.90958221e+00],\n",
    " [3.00306642e-02, 2.62155545e-02, 2.24895160e-02],\n",
    " [2.32018394e+00, 1.70108601e+00, 1.57216678e+00],\n",
    " [4.70959173e-01, 4.07630021e-01, 3.63760006e-01],\n",
    " [5.66143161e-02, 6.20849066e-02, 1.00000000e+02],\n",
    " [3.11922659e+00, 3.08294092e+00, 3.08291001e+00],\n",
    " [4.50745914e-02, 6.04226148e-02, 1.00000000e+02],\n",
    " [3.15928103e+00, 3.17913887e+00, 3.12251508e+00],\n",
    " [3.13860185e+00, 3.13832430e+00, 3.12889946e+00],\n",
    " [5.28808006e-01, 4.35402776e-01, 4.02457457e-01],\n",
    " [5.56639091e-02, 7.52744360e-02, 1.00000000e+02],\n",
    " [3.04484814e+00, 3.06971813e+00, 2.98879607e+00],\n",
    " [3.11096512e+00, 3.11384177e+00, 3.09488298e+00],\n",
    " [4.99300625e-01, 5.08102605e-01, 3.83251740e-01],\n",
    " [4.17841828e-02, 6.20872682e-02, 1.00000000e+02],\n",
    " [3.08642248e+00, 3.13621224e+00, 3.04740250e+00],\n",
    " [3.06899134e+00, 3.05935289e+00, 3.04960891e+00],\n",
    " [4.98806938e-01, 4.38883170e-01, 3.69288010e-01]])\n",
    "\n",
    "relative_gibbs_error = [\n",
    "   error[2] / error[0] for error in errors \n",
    "]\n",
    "print(np.where(errors[:, 2] == 100))\n",
    "\n",
    "error_array = errors[errors[:, 2] != 100]\n",
    "\n",
    "print(f\"Errors for Random forest: {np.mean(error_array[:, 0])}\")\n",
    "print(f\"Errors for XGBoost: {np.mean(error_array[:, 1])}\")\n",
    "print(f\"Errors for Gibbs Forest: {np.mean(error_array[:, 2])}\")\n",
    "\n",
    "print(f\"Fraction of datasets Gibbs Forest is better than Random Forest: {np.mean(error_array[:, 2] < error_array[:, 0])}\")\n",
    "print(f\"Fraction of datasets Gibbs Forest is better than XGBoost: {np.mean(error_array[:, 2] < error_array[:, 1])}\")\n",
    "\n",
    "print(f\"Average relative improvement of Gibbs Forest over Random Forest: {np.mean((error_array[:, 0] - error_array[:, 2]) / error_array[:, 0])}\")\n",
    "print(f\"Average relative improvement of Gibbs Forest over XGBoost: {np.mean((error_array[:, 1] - error_array[:, 2]) / error_array[:, 1])}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations: 0.12322740702600822\n",
      "Tree biases: -0.00042524772461423885\n"
     ]
    }
   ],
   "source": [
    "trees = dyna._trees\n",
    "predictions = [tree.predict(X_test) for tree in trees]\n",
    "tree_biases = np.mean(predictions - y_test, axis = 1)\n",
    "correlations = np.zeros((len(trees), len(trees)))\n",
    "for i in range(len(trees)):\n",
    "    for j in range(i, len(trees)):\n",
    "        correlations[i, j] = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "        correlations[j, i] = correlations[i, j]\n",
    "print(f\"Correlations: {np.mean(correlations)}\")\n",
    "print(f\"Tree biases: {np.mean(tree_biases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GibbsForest' object has no attribute 'estimators_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trees \u001b[38;5;241m=\u001b[39m dyna\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [tree\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m trees]\n\u001b[1;32m      3\u001b[0m tree_biases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(predictions \u001b[38;5;241m-\u001b[39m y_test, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GibbsForest' object has no attribute 'estimators_'"
     ]
    }
   ],
   "source": [
    "trees = dyna.estimators_\n",
    "predictions = [tree.predict(X_test) for tree in trees]\n",
    "tree_biases = np.mean(predictions - y_test, axis = 1)\n",
    "correlations = np.zeros((len(trees), len(trees)))\n",
    "for i in range(len(trees)):\n",
    "    for j in range(i, len(trees)):\n",
    "        correlations[i, j] = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "        correlations[j, i] = correlations[i, j]\n",
    "print(f\"Correlations: {np.mean(correlations)}\")\n",
    "print(f\"Tree biases: {np.mean(tree_biases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------------------------------\n",
    "# Assumptions:\n",
    "#   - dyna: your trained random forest model (e.g., RandomForestRegressor)\n",
    "#           with an attribute `estimators_` (list of individual tree models)\n",
    "#   - X_test: test features (numpynp.array or similar)\n",
    "#   - y_test: true test targets (numpynp.array)\n",
    "#\n",
    "# Replace the following dummy definitions with your actual data/model.\n",
    "# -------------------------------\n",
    "# Example (comment these out if you already have these defined):\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "#\n",
    "# X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# dyna = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "# dyna.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Compute Predictions for Each Tree\n",
    "# -------------------------------\n",
    "trees = dyna._trees\n",
    "n_trees = len(trees)\n",
    "# Each tree produces predictions on the test set; shape: (n_trees, n_samples)\n",
    "predictions = np.array([tree.predict(X_test) for tree in trees])\n",
    "\n",
    "# Ensemble prediction is the average over trees:\n",
    "ensemble_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute Residuals (Errors)\n",
    "# -------------------------------\n",
    "# Residuals for each tree and for the ensemble:\n",
    "residuals = predictions - y_test  # shape: (n_trees, n_samples)\n",
    "ensemble_residual = ensemble_pred - y_test\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Correlation Among Tree Predictions\n",
    "# -------------------------------\n",
    "# Compute the correlation matrix among the treesâ€™ predictions.\n",
    "correlation_matrix = np.corrcoef(predictions, rowvar=False)\n",
    "# Compute the mean correlation (using only the off-diagonal values)\n",
    "upper_tri_indices = np.triu_indices(n_trees, k=1)\n",
    "mean_correlation = np.mean(correlation_matrix[upper_tri_indices])\n",
    "print(\"Mean correlation among tree predictions: {:.4f}\".format(mean_correlation))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Correlation Among Tree Residuals\n",
    "# -------------------------------\n",
    "resid_corr_matrix = np.corrcoef(residuals)\n",
    "mean_resid_corr = np.mean(resid_corr_matrix[upper_tri_indices])\n",
    "print(\"Mean correlation among tree residuals: {:.4f}\".format(mean_resid_corr))\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Compute MSE and MAE for Each Tree and the Ensemble\n",
    "# -------------------------------\n",
    "tree_mse = np.array([mean_squared_error(y_test, predictions[i]) for i in range(n_trees)])\n",
    "tree_mae = np.array([mean_absolute_error(y_test, predictions[i]) for i in range(n_trees)])\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "\n",
    "print(\"Average individual tree MSE: {:.4f}\".format(np.mean(tree_mse)))\n",
    "print(\"Ensemble MSE: {:.4f}\".format(ensemble_mse))\n",
    "print(\"Average individual tree MAE: {:.4f}\".format(np.mean(tree_mae)))\n",
    "print(\"Ensemble MAE: {:.4f}\".format(ensemble_mae))\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Compute Prediction Variance Across Trees (Per Sample)\n",
    "# -------------------------------\n",
    "# For each test sample, compute variance among the tree predictions\n",
    "variance_per_sample = np.var(predictions, axis=0)\n",
    "avg_prediction_variance = np.mean(variance_per_sample)\n",
    "print(\"Average variance of tree predictions across samples: {:.4f}\".format(avg_prediction_variance))\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Ambiguity Decomposition (Regression)\n",
    "# -------------------------------\n",
    "# One way to look at ensemble benefits is via the ambiguity decomposition:\n",
    "#   Ensemble MSE = Average tree MSE - Ambiguity\n",
    "ambiguity = np.mean(tree_mse) - ensemble_mse\n",
    "print(\"Ambiguity (Average tree MSE - Ensemble MSE): {:.4f}\".format(ambiguity))\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Feature Importances (If Available)\n",
    "# -------------------------------\n",
    "# If your individual trees have a `feature_importances_` attribute,\n",
    "# compute the average importance for each feature.\n",
    "try:\n",
    "    feature_importances = np.array([tree.feature_importances_ for tree in trees])\n",
    "    avg_feature_importance = np.mean(feature_importances, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(avg_feature_importance)), avg_feature_importance, color='skyblue')\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Average Feature Importance\")\n",
    "    plt.title(\"Average Feature Importances Across Trees\")\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    print(\"Not all trees have a 'feature_importances_' attribute.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Plot Distribution of Residuals\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot ensemble residual distribution\n",
    "sns.histplot(ensemble_residual, color=\"blue\", label=\"Ensemble Residual\", kde=True, stat=\"density\", bins=30)\n",
    "\n",
    "# Plot residual distributions for a few individual trees (first 3 trees)\n",
    "for i in [38, 39]:\n",
    "    sns.histplot(residuals[i], kde=True, stat=\"density\", bins=30, label=f\"Tree {i} Residual\", alpha=0.6)\n",
    "    \n",
    "plt.xlabel(\"Residual (Prediction - True Value)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Residual Distributions: Ensemble vs. Individual Trees\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Correlation Heatmap for Tree Predictions\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Heatmap of Tree Predictions Correlations\")\n",
    "plt.xlabel(\"Tree Index\")\n",
    "plt.ylabel(\"Tree Index\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Correlation Heatmap for Tree Residuals\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(resid_corr_matrix, cmap=\"coolwarm\", square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Heatmap of Tree Residuals Correlations\")\n",
    "plt.xlabel(\"Tree Index\")\n",
    "plt.ylabel(\"Tree Index\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Boxplots for Individual Tree MSE and MAE\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=tree_mse)\n",
    "plt.title(\"Boxplot of Individual Tree MSE\")\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=tree_mae)\n",
    "plt.title(\"Boxplot of Individual Tree MAE\")\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_counts = [tree.num_splits for tree in dyna._trees]\n",
    "plt.hist(split_counts, bins=range(0, max(split_counts)+2, 2))\n",
    "plt.title(\"Distribution of Splits per Tree\")\n",
    "plt.xlabel(\"Number of Splits\")\n",
    "plt.ylabel(\"Count of Trees\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = dyna._trees\n",
    "predictions = np.array([tree.predict(X_train) for tree in trees])\n",
    "tree_mse = np.array([mean_squared_error(y_train, predictions[i]) for i in range(len(trees))])\n",
    "best_tree_idxs = np.argsort(tree_mse)\n",
    "errors = []\n",
    "for num_trees in range(2, 79):    \n",
    "    tree_list = [trees[i] for i in best_tree_idxs][:num_trees]\n",
    "    predictions_test = np.array([tree.predict(X_test) for tree in tree_list])\n",
    "    ensemble_pred = np.mean(predictions_test, axis=0)\n",
    "    ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "    ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "    errors.append(ensemble_mae)\n",
    "\n",
    "# Plot the error as a function of the number of trees\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 79), errors, marker='o', color='blue')\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Ensemble MAE\")\n",
    "plt.title(\"Ensemble MAE vs. Number of Trees\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "print(xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
