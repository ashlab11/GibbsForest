{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from openml.tasks import list_tasks, TaskType\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from src import GibbsForest\n",
    "import logging\n",
    "\n",
    "\n",
    "# Load the OpenML dataset\n",
    "regression_tasks = list_tasks(task_type = TaskType.SUPERVISED_REGRESSION)\n",
    "small_tasks_ids = []\n",
    "for task_id, task_value in regression_tasks.items():\n",
    "    \"\"\"We want datasets with instances between 5000 and 10000, no missing values, and no symbolic features\"\"\"\n",
    "    if ('NumberOfInstances' in task_value.keys() and task_value['NumberOfInstances'] < 10000 and task_value['NumberOfInstances'] > 5000 and\n",
    "    'NumberOfMissingValues' in task_value.keys() and task_value['NumberOfMissingValues'] == 0 and \n",
    "    'NumberOfSymbolicFeatures' in task_value.keys() and task_value['NumberOfSymbolicFeatures'] == 0):\n",
    "        small_tasks_ids.append(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"openml.extensions.sklearn.extension\").setLevel(logging.ERROR)\n",
    "if not hasattr(GibbsForest, '__version__'):\n",
    "    GibbsForest.__version__ = \"0.0.1\"  # Use an appropriate version number\n",
    "\n",
    "print(len(small_tasks_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y mean: 83.9689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src import Losses\n",
    "\n",
    "task_id = small_tasks_ids[20]\n",
    "task = openml.tasks.get_task(task_id)\n",
    "X, y = task.get_X_and_y()\n",
    "print(f\"Y mean: {y.mean():.4f}\")\n",
    "\n",
    "gibbs_params = {\"eta\": 0.1,\n",
    "            \"feature_subsample\": 0.6,\n",
    "            \"max_depth\": 5,\n",
    "            \"min_samples\": 2,\n",
    "            \"n_trees\": 80, \n",
    "            'row_subsample': 0.9, \n",
    "            'warmup_depth': 2, \n",
    "            'loss_fn': Losses.LeastSquaresLoss(), \n",
    "            'reg_lambda': 0,\n",
    "            'reg_gamma': 1}\n",
    "\n",
    "dyna = GibbsForest(**gibbs_params)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.1, \n",
    "    'subsample': 0.99, \n",
    "    'max_depth': 5, \n",
    "    'reg_lambda': 0.1, \n",
    "    'gamma': 1\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\"\"\"rf_params = {\n",
    "            \"max_depth\": 5,\n",
    "            \"max_features\": 0.50,\n",
    "            \"min_samples_leaf\": 3,\n",
    "            \"n_estimators\": 68}\n",
    "dyna = RandomForestRegressor(**rf_params)\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "dyna.fit(X_train, y_train)\n",
    "train_error = mean_absolute_error(y_train, dyna.predict(X_train))\n",
    "test_error = mean_absolute_error(y_test, dyna.predict(X_test))\n",
    "print(f\"Train error: {train_error:.4f}\")\n",
    "print(f\"Test error: {test_error:.4f}\")\n",
    "\"\"\"booster = dyna.get_booster()\n",
    "df = booster.trees_to_dataframe()\n",
    "total_leaves = (df[\"Feature\"] == \"Leaf\").sum()\n",
    "print(f\"Total leaves: {total_leaves}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([14, 30, 36, 38, 42, 46]),)\n",
      "Errors for Random forest: 1.3007481757745454\n",
      "Errors for XGBoost: 1.1813441520618182\n",
      "Errors for Gibbs Forest: 1.1316849013131818\n",
      "Fraction of datasets Gibbs Forest is better than Random Forest: 0.7954545454545454\n",
      "Fraction of datasets Gibbs Forest is better than XGBoost: 0.8636363636363636\n",
      "Average relative improvement of Gibbs Forest over Random Forest: 0.1256627863698241\n",
      "Average relative improvement of Gibbs Forest over XGBoost: 0.05585898940360959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "errors = np.array([[1.55012961e-01, 1.19690567e-01, 1.18715062e-01],\n",
    " [2.27864006e+00, 1.63303653e+00, 1.59774050e+00],\n",
    " [1.11279170e-03, 1.30388785e-03, 1.12549638e-03],\n",
    " [2.61028864e+00, 2.56309775e+00, 2.53819391e+00],\n",
    " [2.54181707e+00, 2.28245487e+00, 1.87539849e+00],\n",
    " [5.17212119e-01, 4.22303067e-01, 4.00673763e-01],\n",
    " [5.00497202e-01, 4.19176071e-01, 3.75706172e-01],\n",
    " [4.87178472e-01, 4.36459287e-01, 3.70631891e-01],\n",
    " [5.61965561e-01, 5.18073543e-01, 4.96628538e-01],\n",
    " [1.07452048e-01, 1.08739641e-01, 1.08610724e-01],\n",
    " [8.42492473e-03, 9.02710288e-03, 8.74388075e-03],\n",
    " [1.85544239e-02, 1.86710448e-02, 1.86979661e-02],\n",
    " [1.93822924e-02, 1.93739274e-02, 1.93643104e-02],\n",
    " [2.66070075e+00, 2.47643256e+00, 2.43504228e+00],\n",
    " [1.00000000e+02, 1.00000000e+02, 1.00000000e+02],\n",
    " [5.87860977e-02, 5.53437466e-02, 5.47524916e-02],\n",
    " [2.52429177e+00, 2.10119247e+00, 1.90582350e+00],\n",
    " [2.96629201e-02, 2.45982695e-02, 2.26927044e-02],\n",
    " [2.37716708e+00, 1.73599986e+00, 1.56691770e+00],\n",
    " [1.53521432e-01, 1.14978454e-01, 1.09412079e-01],\n",
    " [2.42959334e+00, 1.68032178e+00, 1.69157321e+00],\n",
    " [1.06947566e-03, 1.27401975e-03, 1.12249952e-03],\n",
    " [2.55821045e+00, 2.54205453e+00, 2.56533770e+00],\n",
    " [2.48475628e+00, 2.07870051e+00, 1.90476740e+00],\n",
    " [5.29935374e-01, 5.12014280e-01, 4.84926766e-01],\n",
    " [1.09046146e-01, 1.09390472e-01, 1.11111156e-01],\n",
    " [8.44670889e-03, 8.59229324e-03, 7.76171153e-03],\n",
    " [1.98315069e-02, 1.99174567e-02, 1.98971291e-02],\n",
    " [1.99302548e-02, 1.99821859e-02, 2.00077061e-02],\n",
    " [2.70376614e+00, 2.35912366e+00, 2.37085021e+00],\n",
    " [1.00000000e+02, 1.00000000e+02, 1.00000000e+02],\n",
    " [5.73799461e-02, 5.55275076e-02, 5.38979819e-02],\n",
    " [2.62285960e+00, 2.23544209e+00, 1.90958221e+00],\n",
    " [3.00306642e-02, 2.62155545e-02, 2.24895160e-02],\n",
    " [2.32018394e+00, 1.70108601e+00, 1.57216678e+00],\n",
    " [4.70959173e-01, 4.07630021e-01, 3.63760006e-01],\n",
    " [5.66143161e-02, 6.20849066e-02, 1.00000000e+02],\n",
    " [3.11922659e+00, 3.08294092e+00, 3.08291001e+00],\n",
    " [4.50745914e-02, 6.04226148e-02, 1.00000000e+02],\n",
    " [3.15928103e+00, 3.17913887e+00, 3.12251508e+00],\n",
    " [3.13860185e+00, 3.13832430e+00, 3.12889946e+00],\n",
    " [5.28808006e-01, 4.35402776e-01, 4.02457457e-01],\n",
    " [5.56639091e-02, 7.52744360e-02, 1.00000000e+02],\n",
    " [3.04484814e+00, 3.06971813e+00, 2.98879607e+00],\n",
    " [3.11096512e+00, 3.11384177e+00, 3.09488298e+00],\n",
    " [4.99300625e-01, 5.08102605e-01, 3.83251740e-01],\n",
    " [4.17841828e-02, 6.20872682e-02, 1.00000000e+02],\n",
    " [3.08642248e+00, 3.13621224e+00, 3.04740250e+00],\n",
    " [3.06899134e+00, 3.05935289e+00, 3.04960891e+00],\n",
    " [4.98806938e-01, 4.38883170e-01, 3.69288010e-01]])\n",
    "\n",
    "relative_gibbs_error = [\n",
    "   error[2] / error[0] for error in errors \n",
    "]\n",
    "print(np.where(errors[:, 2] == 100))\n",
    "\n",
    "error_array = errors[errors[:, 2] != 100]\n",
    "\n",
    "print(f\"Errors for Random forest: {np.mean(error_array[:, 0])}\")\n",
    "print(f\"Errors for XGBoost: {np.mean(error_array[:, 1])}\")\n",
    "print(f\"Errors for Gibbs Forest: {np.mean(error_array[:, 2])}\")\n",
    "\n",
    "print(f\"Fraction of datasets Gibbs Forest is better than Random Forest: {np.mean(error_array[:, 2] < error_array[:, 0])}\")\n",
    "print(f\"Fraction of datasets Gibbs Forest is better than XGBoost: {np.mean(error_array[:, 2] < error_array[:, 1])}\")\n",
    "\n",
    "print(f\"Average relative improvement of Gibbs Forest over Random Forest: {np.mean((error_array[:, 0] - error_array[:, 2]) / error_array[:, 0])}\")\n",
    "print(f\"Average relative improvement of Gibbs Forest over XGBoost: {np.mean((error_array[:, 1] - error_array[:, 2]) / error_array[:, 1])}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trees \u001b[38;5;241m=\u001b[39m dyna\u001b[38;5;241m.\u001b[39m_trees\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [tree\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m trees]\n\u001b[0;32m----> 3\u001b[0m tree_biases \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(predictions \u001b[38;5;241m-\u001b[39m y_test, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m correlations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(trees), \u001b[38;5;28mlen\u001b[39m(trees)))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trees)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "trees = dyna._trees\n",
    "predictions = [tree.predict(X_test) for tree in trees]\n",
    "tree_biases = np.mean(predictions - y_test, axis = 1)\n",
    "correlations = np.zeros((len(trees), len(trees)))\n",
    "for i in range(len(trees)):\n",
    "    for j in range(i, len(trees)):\n",
    "        correlations[i, j] = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "        correlations[j, i] = correlations[i, j]\n",
    "print(f\"Correlations: {np.mean(correlations)}\")\n",
    "print(f\"Tree biases: {np.mean(tree_biases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = dyna.estimators_\n",
    "predictions = [tree.predict(X_test) for tree in trees]\n",
    "tree_biases = np.mean(predictions - y_test, axis = 1)\n",
    "correlations = np.zeros((len(trees), len(trees)))\n",
    "for i in range(len(trees)):\n",
    "    for j in range(i, len(trees)):\n",
    "        correlations[i, j] = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "        correlations[j, i] = correlations[i, j]\n",
    "print(f\"Correlations: {np.mean(correlations)}\")\n",
    "print(f\"Tree biases: {np.mean(tree_biases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------------------------------\n",
    "# Assumptions:\n",
    "#   - dyna: your trained random forest model (e.g., RandomForestRegressor)\n",
    "#           with an attribute `estimators_` (list of individual tree models)\n",
    "#   - X_test: test features (numpynp.array or similar)\n",
    "#   - y_test: true test targets (numpynp.array)\n",
    "#\n",
    "# Replace the following dummy definitions with your actual data/model.\n",
    "# -------------------------------\n",
    "# Example (comment these out if you already have these defined):\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "#\n",
    "# X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# dyna = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "# dyna.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Compute Predictions for Each Tree\n",
    "# -------------------------------\n",
    "trees = dyna._trees\n",
    "n_trees = len(trees)\n",
    "# Each tree produces predictions on the test set; shape: (n_trees, n_samples)\n",
    "predictions = np.array([tree.predict(X_test) for tree in trees])\n",
    "\n",
    "# Ensemble prediction is the average over trees:\n",
    "ensemble_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute Residuals (Errors)\n",
    "# -------------------------------\n",
    "# Residuals for each tree and for the ensemble:\n",
    "residuals = predictions - y_test  # shape: (n_trees, n_samples)\n",
    "ensemble_residual = ensemble_pred - y_test\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Correlation Among Tree Predictions\n",
    "# -------------------------------\n",
    "# Compute the correlation matrix among the treesâ€™ predictions.\n",
    "correlation_matrix = np.corrcoef(predictions, rowvar=False)\n",
    "# Compute the mean correlation (using only the off-diagonal values)\n",
    "upper_tri_indices = np.triu_indices(n_trees, k=1)\n",
    "mean_correlation = np.mean(correlation_matrix[upper_tri_indices])\n",
    "print(\"Mean correlation among tree predictions: {:.4f}\".format(mean_correlation))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Correlation Among Tree Residuals\n",
    "# -------------------------------\n",
    "resid_corr_matrix = np.corrcoef(residuals)\n",
    "mean_resid_corr = np.mean(resid_corr_matrix[upper_tri_indices])\n",
    "print(\"Mean correlation among tree residuals: {:.4f}\".format(mean_resid_corr))\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Compute MSE and MAE for Each Tree and the Ensemble\n",
    "# -------------------------------\n",
    "tree_mse = np.array([mean_squared_error(y_test, predictions[i]) for i in range(n_trees)])\n",
    "tree_mae = np.array([mean_absolute_error(y_test, predictions[i]) for i in range(n_trees)])\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "\n",
    "print(\"Average individual tree MSE: {:.4f}\".format(np.mean(tree_mse)))\n",
    "print(\"Ensemble MSE: {:.4f}\".format(ensemble_mse))\n",
    "print(\"Average individual tree MAE: {:.4f}\".format(np.mean(tree_mae)))\n",
    "print(\"Ensemble MAE: {:.4f}\".format(ensemble_mae))\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Compute Prediction Variance Across Trees (Per Sample)\n",
    "# -------------------------------\n",
    "# For each test sample, compute variance among the tree predictions\n",
    "variance_per_sample = np.var(predictions, axis=0)\n",
    "avg_prediction_variance = np.mean(variance_per_sample)\n",
    "print(\"Average variance of tree predictions across samples: {:.4f}\".format(avg_prediction_variance))\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Ambiguity Decomposition (Regression)\n",
    "# -------------------------------\n",
    "# One way to look at ensemble benefits is via the ambiguity decomposition:\n",
    "#   Ensemble MSE = Average tree MSE - Ambiguity\n",
    "ambiguity = np.mean(tree_mse) - ensemble_mse\n",
    "print(\"Ambiguity (Average tree MSE - Ensemble MSE): {:.4f}\".format(ambiguity))\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Feature Importances (If Available)\n",
    "# -------------------------------\n",
    "# If your individual trees have a `feature_importances_` attribute,\n",
    "# compute the average importance for each feature.\n",
    "try:\n",
    "    feature_importances = np.array([tree.feature_importances_ for tree in trees])\n",
    "    avg_feature_importance = np.mean(feature_importances, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(avg_feature_importance)), avg_feature_importance, color='skyblue')\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Average Feature Importance\")\n",
    "    plt.title(\"Average Feature Importances Across Trees\")\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    print(\"Not all trees have a 'feature_importances_' attribute.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Plot Distribution of Residuals\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot ensemble residual distribution\n",
    "sns.histplot(ensemble_residual, color=\"blue\", label=\"Ensemble Residual\", kde=True, stat=\"density\", bins=30)\n",
    "\n",
    "# Plot residual distributions for a few individual trees (first 3 trees)\n",
    "for i in [38, 39]:\n",
    "    sns.histplot(residuals[i], kde=True, stat=\"density\", bins=30, label=f\"Tree {i} Residual\", alpha=0.6)\n",
    "    \n",
    "plt.xlabel(\"Residual (Prediction - True Value)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Residual Distributions: Ensemble vs. Individual Trees\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Correlation Heatmap for Tree Predictions\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Heatmap of Tree Predictions Correlations\")\n",
    "plt.xlabel(\"Tree Index\")\n",
    "plt.ylabel(\"Tree Index\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Correlation Heatmap for Tree Residuals\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(resid_corr_matrix, cmap=\"coolwarm\", square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Heatmap of Tree Residuals Correlations\")\n",
    "plt.xlabel(\"Tree Index\")\n",
    "plt.ylabel(\"Tree Index\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Boxplots for Individual Tree MSE and MAE\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=tree_mse)\n",
    "plt.title(\"Boxplot of Individual Tree MSE\")\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=tree_mae)\n",
    "plt.title(\"Boxplot of Individual Tree MAE\")\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_counts = [tree.num_splits for tree in dyna._trees]\n",
    "plt.hist(split_counts, bins=range(0, max(split_counts)+2, 2))\n",
    "plt.title(\"Distribution of Splits per Tree\")\n",
    "plt.xlabel(\"Number of Splits\")\n",
    "plt.ylabel(\"Count of Trees\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = dyna._trees\n",
    "predictions = np.array([tree.predict(X_train) for tree in trees])\n",
    "tree_mse = np.array([mean_squared_error(y_train, predictions[i]) for i in range(len(trees))])\n",
    "best_tree_idxs = np.argsort(tree_mse)\n",
    "errors = []\n",
    "for num_trees in range(2, 79):    \n",
    "    tree_list = [trees[i] for i in best_tree_idxs][:num_trees]\n",
    "    predictions_test = np.array([tree.predict(X_test) for tree in tree_list])\n",
    "    ensemble_pred = np.mean(predictions_test, axis=0)\n",
    "    ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "    ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "    errors.append(ensemble_mae)\n",
    "\n",
    "# Plot the error as a function of the number of trees\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 79), errors, marker='o', color='blue')\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Ensemble MAE\")\n",
    "plt.title(\"Ensemble MAE vs. Number of Trees\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "print(xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
